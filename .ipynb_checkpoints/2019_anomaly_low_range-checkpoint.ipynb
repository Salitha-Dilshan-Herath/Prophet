{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time-series outlier detection using Prophet on weather data\n",
    "\n",
    "## Method\n",
    "\n",
    "The Prophet outlier detector uses the [Prophet](https://facebook.github.io/prophet/) time series forecasting package explained in [this excellent paper](https://peerj.com/preprints/3190/). The underlying Prophet model is a decomposable univariate time series model combining trend, seasonality and holiday effects. The model forecast also includes an uncertainty interval around the estimated trend component using the [MAP estimate](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) of the extrapolated model. Alternatively, full Bayesian inference can be done at the expense of increased compute. The upper and lower values of the uncertainty interval can then be used as outlier thresholds for each point in time. First, the distance from the observed value to the nearest uncertainty boundary (upper or lower) is computed. If the observation is within the boundaries, the outlier score equals the negative distance. As a result, the outlier score is the lowest when the observation equals the model prediction. If the observation is outside of the boundaries, the score equals the distance measure and the observation is flagged as an outlier. One of the main drawbacks of the method however is that you need to refit the model as new data comes in. This is undesirable for applications with high throughput and real-time detection.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The example uses a weather time series dataset recorded by the [Max-Planck-Institute for Biogeochemistry](https://www.bgc-jena.mpg.de/wetter/). The dataset contains 14 different features such as air temperature, atmospheric pressure, and humidity. These were collected every 10 minutes, beginning in 2003. Like the [TensorFlow time-series tutorial](https://www.tensorflow.org/tutorials/structured_data/time_series), we only use data collected between 2009 and 2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "from alibi_detect.od import OutlierProphet\n",
    "from alibi_detect.utils.fetching import fetch_detector\n",
    "from alibi_detect.utils.saving import save_detector, load_detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess log file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv('data/413377922610_elasticloadbalancing_us-east-1_prod_20200703T0958Z_54.243.202.15_3acrxxhj.log', sep=' ', header=None, encoding='utf-8')\n",
    "log_df.columns = [\n",
    "            'timestamp',\n",
    "            'elb',\n",
    "            'client',\n",
    "            'backend',\n",
    "            'request_processing_time',\n",
    "            'backend_processing_time',\n",
    "            'response_processing_time',\n",
    "            'elb_status_code',\n",
    "            'backend_status_code',\n",
    "            'received_bytes',\n",
    "            'sent_bytes',\n",
    "            'ssl_cipher',\n",
    "            'ssl_protocol',\n",
    "            'request',\n",
    "            'user_agent'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rescale_df = log_df.filter(['timestamp', 'backend_processing_time'], axis=1)\n",
    "\n",
    "rescale_df['timestamp'] = rescale_df['timestamp'].apply(lambda x: (datetime.strptime(x, '%Y-%m-%dT%H:%M:%S.%fZ')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>backend_processing_time</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>timestamp</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>33496.666667</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:15:00</th>\n",
       "      <td>33292.000000</td>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:30:00</th>\n",
       "      <td>33368.000000</td>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:45:00</th>\n",
       "      <td>33396.000000</td>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-01-01 01:00:00</th>\n",
       "      <td>33312.666667</td>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     backend_processing_time           timestamp\n",
       "timestamp                                                       \n",
       "2019-01-01 00:00:00             33496.666667 2019-01-01 00:00:00\n",
       "2019-01-01 00:15:00             33292.000000 2019-01-01 00:15:00\n",
       "2019-01-01 00:30:00             33368.000000 2019-01-01 00:30:00\n",
       "2019-01-01 00:45:00             33396.000000 2019-01-01 00:45:00\n",
       "2019-01-01 01:00:00             33312.666667 2019-01-01 01:00:00"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1 = rescale_df\n",
    "df_1['timestamp'] = pd.to_datetime(df_1['timestamp'], format=\"%Y-%m-%dT%H:%M:%S\")\n",
    "# df_1.index = df_1.timestamp\n",
    "df_2 = df_1.resample('15Min', on='timestamp').mean()\n",
    "df_2['timestamp'] = df_2.index\n",
    "df_2.to_csv('train.csv')\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('train.csv', index_col=None)\n",
    "df.drop(['timestamp.1'],axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>backend_processing_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>33496.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2019-01-01 00:15:00</td>\n",
       "      <td>33292.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2019-01-01 00:30:00</td>\n",
       "      <td>33368.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2019-01-01 00:45:00</td>\n",
       "      <td>33396.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2019-01-01 01:00:00</td>\n",
       "      <td>33312.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             timestamp  backend_processing_time\n",
       "0  2019-01-01 00:00:00             33496.666667\n",
       "1  2019-01-01 00:15:00             33292.000000\n",
       "2  2019-01-01 00:30:00             33368.000000\n",
       "3  2019-01-01 00:45:00             33396.000000\n",
       "4  2019-01-01 01:00:00             33312.666667"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zip_path = tf.keras.utils.get_file(\n",
    "#     origin='https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip',\n",
    "#     fname='jena_climate_2009_2016.csv.zip',\n",
    "#     extract=True\n",
    "# )\n",
    "# csv_path, _ = os.path.splitext(zip_path)\n",
    "# df = pd.read_csv(csv_path)\n",
    "# df['Date Time'] = pd.to_datetime(df['Date Time'], format='%d.%m.%Y %H:%M:%S')\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select subset to test Prophet model on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_prophet = 30040"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet model expects a DataFrame with 2 columns: one named ```ds``` with the timestamps and one named ```y``` with the time series to be evaluated. We will just look at the temperature data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30040, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ds</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>30035</th>\n",
       "      <td>2019-11-09 20:45:00</td>\n",
       "      <td>33457.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30036</th>\n",
       "      <td>2019-11-09 21:00:00</td>\n",
       "      <td>33575.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30037</th>\n",
       "      <td>2019-11-09 21:15:00</td>\n",
       "      <td>33516.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30038</th>\n",
       "      <td>2019-11-09 21:30:00</td>\n",
       "      <td>33405.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30039</th>\n",
       "      <td>2019-11-09 21:45:00</td>\n",
       "      <td>33428.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        ds             y\n",
       "30035  2019-11-09 20:45:00  33457.333333\n",
       "30036  2019-11-09 21:00:00  33575.333333\n",
       "30037  2019-11-09 21:15:00  33516.666667\n",
       "30038  2019-11-09 21:30:00  33405.333333\n",
       "30039  2019-11-09 21:45:00  33428.000000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'ds': df['timestamp'][:n_prophet], 'y': df['backend_processing_time'][:n_prophet]}\n",
    "df_T = pd.DataFrame(data=d)\n",
    "print(df_T.shape)\n",
    "#df_T.head()\n",
    "\n",
    "df_T.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_T['ds'], df_T['y'])\n",
    "plt.title('backend_processing_time over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('T (time)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load or define outlier detector\n",
    "\n",
    "The pretrained outlier and adversarial detectors used in the example notebooks can be found [here](https://console.cloud.google.com/storage/browser/seldon-models/alibi-detect). You can use the built-in ```fetch_detector``` function which saves the pre-trained models in a local directory ```filepath``` and loads the detector. Alternatively, you can train a detector from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_outlier_detector = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'my_path'  # change to directory where model is downloaded\n",
    "if load_outlier_detector:  # load pretrained outlier detector\n",
    "    detector_type = 'outlier'\n",
    "    dataset = 'weather'\n",
    "    detector_name = 'OutlierProphet'\n",
    "    od = fetch_detector(filepath, detector_type, dataset, detector_name)\n",
    "    filepath = os.path.join(filepath, detector_name)\n",
    "else:  # initialize, fit and save outlier detector\n",
    "    od = OutlierProphet(yearly_seasonality = True, weekly_seasonality= True, daily_seasonality = True, threshold=.9)\n",
    "    od.fit(df_T)\n",
    "    save_detector(od, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please check out the [documentation](https://docs.seldon.io/projects/alibi-detect/en/latest/methods/prophet.html) as well as the original [Prophet documentation](https://facebook.github.io/prophet/) on how to customize the Prophet-based outlier detector and add seasonalities, holidays, opt for a saturating logistic growth model or apply parameter regularization.\n",
    "\n",
    "## Predict outliers on test data\n",
    "\n",
    "Define the test data. It is important that the timestamps of the test data follow the training data. We check this below by comparing the first few rows of the test DataFrame with the last few of the training DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_periods = 4000\n",
    "d = {'ds': df['timestamp'][n_prophet:n_prophet+n_periods], \n",
    "     'y': df['backend_processing_time'][n_prophet:n_prophet+n_periods]}\n",
    "df_T_test = pd.DataFrame(data=d)\n",
    "df_T_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_T.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict outliers on test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_preds = od.predict(\n",
    "    df_T_test, \n",
    "    return_instance_score=True,\n",
    "    return_forecast=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "\n",
    "We can first visualize our predictions with Prophet's built in plotting functionality. This also allows us to include historical predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future = od.model.make_future_dataframe(periods=500, freq='15T', include_history=True)\n",
    "forecast = od.model.predict(future)\n",
    "fig = od.model.plot(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the breakdown of the different components in the forecast. Since we did not do full Bayesian inference with `mcmc_samples`, the uncertaintly intervals of the forecast are determined by the [MAP estimate](https://en.wikipedia.org/wiki/Maximum_a_posteriori_estimation) of the extrapolated trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig = od.model.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the further we predict in the future, the wider the uncertainty intervals which determine the outlier threshold.\n",
    "\n",
    "Let's overlay the actual data with the upper and lower outlier thresholds predictions and check where we predicted outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast['y'] = df['backend_processing_time'][:n_prophet+n_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast[['ds', 'yhat', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.register_matplotlib_converters()  # needed to plot timestamps\n",
    "forecast[-2976:].plot(x='ds', y=['y', 'yhat', 'yhat_upper', 'yhat_lower'])\n",
    "plt.title('Predicted T (in °C) over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('T (in °C)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outlier scores and predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "od_preds['data']['forecast']['threshold'] = np.zeros(n_periods)\n",
    "od_preds['data']['forecast'][-n_periods:].plot(x='ds', y=['score', 'threshold'])\n",
    "plt.title('Outlier score over time')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Outlier score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier scores naturally trend down as uncertainty increases when we predict further in the future.\n",
    "\n",
    "Let's look at some individual outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fcst = od_preds['data']['forecast']\n",
    "df_outlier = df_fcst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of outliers: {}'.format(df_outlier.shape[0]))\n",
    "df_outlier[['ds', 'yhat', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
